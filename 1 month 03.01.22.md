# 1ый месяц

### Предисловие
Решил сегодня _(03.01.2021)_ начать писать дневник. Буду выражать здесь свои мысли, успехи за день.
И для тренировки скоропечатания неплохо и для структуризации мыслей тоже. 
Это уже, наверное, третья моя попытка начать и систематически продолжать нечто подобное.
Ну чтож, надеюсь в этот раз мне будет не лень и будет время. Чтож в добрый путь!

## 03.01.2022

---
Сегодня снова вернулся к заброшенному курсу по ML от МФТИ. Пока вспоминал все что,
забыл и пытался реализовать predict и accuracy средствами PyTorch.
Также понял как работать с DataLoader: это по сути утилита, которая может делить 
переданный ей датасет на батчи. 

## 05.01.2022

---
Разобрался что такое тензоры в Pytorch. До этого просто принимал их как данное. Оказывается это тот
же Numpy array, но только в отличие от него может вычисляться на видеокарте. Также почитал инфу 
про автоматическое вычисление градиентов ([Autograd](https://www.youtube.com/watch?v=MswxJw-8PvE&ab_channel=ElliotWaite)).
Pytorch в отличие от остальных фреймворков, вычисляет их динамически. Т.е. граф вычислений
строиться прямо во время вычисления. Это позволяет с помощью правила нахождения производных 
сложных функций ("Правило цепочек") вычислять производные, а соответственно градиенты. 
Вызывается это с помощью метода backward() к функции которую надо оптимизировать.
Например, в NN это будет функция потерь - loss. Пусть это будет MSE = ((y-yb)**2).mean(). 
Тогда, чтобы найти градиент, нужно просто вызвать loss.backward(). 

Понял для чего нужен метод forward при создании своей нейронной сети. При создании класса NN
нужно наследоваться от базового класса nn.Module. Там при инициализации задается архитектура, и 
чтобы задать принципы перемещения данных по ней, реализуется метод forward(input). 
Там буквально описывается процесс перехода данных из одного слоя в другой. Например, результатом
работы метода на задаче классификации будет распределение вероятностей по классам. 
Этот метод вызывается при передаче значений нашей нейронной сети.
```python
net_out = net(data) # <- вот тут вызывается метод forward
loss = criterion(net_out, target)
```

## 06.01.2022

---
Наконец получилось установить CUDA и PyTorch ее [увидел](https://pytorch.org/get-started/locally/). 
Также пытался запустить вычисления на GPU, но я так и не понял получилось ли. Видеокарточка 
была загружена всего на 2% в то время как процессор на 7%. Возможно это так получается из-за
того, что я считаю градиенты по малым батчам, а не сую целый датасет в память.
Надо будет это проверить.

## 07.01.2022

---
Все таки я все правильно подключил и все вычислялось на GPU. Как я понял, из-за того, что модель
была само по себе простая, то и видеокарта почти не нагружалась. Но то что она работала можно было
судить по энергопотреблению в диспетчере задач. Это дополнительно подтвердилось когда я реализовал
LeNet и загрузка видеокарты достигала в среднем 13%. И да, я НАКОНЕЦ-ТО доделал это несчастное 
задание по сверточным нейронным сетям.

## 08.01.2022

---
Для меня стало открытием вся логика работы _Обратного распростарненния ошибки_. До этого я идейно 
понимал как оно работает с производными и понимал пример на одном нейроне, но не задумывался, а
как это все работает в обычных многослойных сетях. В общем идея работы следующая (разберем на 
многоклассовой классификации):

1. Пусть у нас подается на сеть 10 признаков, а на выходе будет **список** вероятностей отнесения к 5 классам
2. Тогда чтобы посчитать градиенты, нужно оптимизировать функцию потерь. Так как до этого я считал,
что это просто разность между правильный ответом и ответом сети, то тогда это была бы какая-то такая
функция: L = y - max(y1, y2, y3, y4, y5). Но max - недифференцируемая функция. 
3. Эта функция работала бы в задаче регрессии (например, предсказания следующего значения последовательности),
так как там на выходе есть только одно значение! 
4. Поэтому в задаче многоклассовой классификации используют Логистическую регрессию:  
   
    > $Cost(h_\theta(x), y) = -y log(h_\theta(x)) - (1 - y)log(h_\theta(x))$ - пример для двух классов, которая возвращает число - на сколько сеть далеко от правильного ответа!
    
5. Но это еще не все. Так как ответ нейронной сети (для конкретики пусть это будет y1)
это какая-то сложная функция: сумма произведения выходов предыдущего слоя и соответствующих весов,
то получается что функция потерь - это такая матрешка из сложных функций:

    > y1 = sum(z1*w1 + z2*w2 + ... + zn*wn),  
    > где n - кол-во нейронов на предыдущем слое, zi - ответ i нейрона, wi - вес на i нейроне;  
    > в свою очередь z1 (как и остальные z) - тоже сложная функция = sum(zz1*ww1 + zz2*ww2 + ... + zzk*wwk),  
    > где k - кол-во нейронов на предыдущем слое, zzi - ответ i нейрона, wwi - вес на i нейроне;
    > и так далее...

6. И так вычисляя градиенты и все глубже спускаясь по функции, можно дойти до входных значений,
а значит в итоге найти градиент функции потерь по ВСЕМ весам на каждом слое нейронной сети.
